MACHINE LEARNING INFORMATION GUIDE
=============================

INTRODUCTION TO MACHINE LEARNING
-------------------------------

Machine learning is a field of artificial intelligence that uses statistical techniques to give computer systems the ability to 'learn' from data without being explicitly programmed. Machine learning algorithms build mathematical models based on sample data to make predictions or decisions.

Key Machine Learning Paradigms:
- Supervised Learning: The algorithm learns from labeled training data, and makes predictions based on that data.
- Unsupervised Learning: The algorithm learns patterns from unlabeled data.
- Reinforcement Learning: The algorithm learns by interacting with an environment and receiving rewards or penalties.

Common Applications:
- Image and speech recognition
- Natural language processing
- Recommendation systems
- Medical diagnosis
- Financial forecasting
- Autonomous vehicles


NEURAL NETWORKS
--------------

Neural networks are computing systems inspired by the biological neural networks in animal brains. They consist of artificial neurons that can learn from and make decisions based on data.

Key Concepts:
- Neurons: Basic computational units that process inputs and activate based on thresholds.
- Layers: Networks typically have an input layer, hidden layers, and an output layer.
- Weights: Parameters adjusted during training to improve predictions.
- Activation Functions: Functions that determine if a neuron should be activated (e.g., ReLU, Sigmoid, Tanh).

Types of Neural Networks:
- Convolutional Neural Networks (CNNs): Specially designed for processing grid-like data such as images.
- Recurrent Neural Networks (RNNs): Process sequential data by maintaining a memory of previous inputs.
- Long Short-Term Memory Networks (LSTMs): Special RNNs that can learn long-term dependencies.
- Generative Adversarial Networks (GANs): Consist of two networks that compete: a generator and a discriminator.

Applications:
- Image and video recognition
- Natural language processing
- Speech recognition
- Medical diagnosis
- Game playing
- Financial forecasting


MACHINE LEARNING ALGORITHMS
--------------------------

Supervised Learning Algorithms:
- Linear Regression: Predicts a continuous output based on input features using a linear relationship.
- Logistic Regression: Used for binary classification, estimates the probability of an instance belonging to a class.
- Decision Trees: Models decisions as a tree of choices that lead to outcomes.
- Random Forests: Ensemble method combining multiple decision trees to improve accuracy and reduce overfitting.
- Support Vector Machines (SVM): Finds the optimal hyperplane that separates classes in the feature space.
- K-Nearest Neighbors (KNN): Classifies data points based on the majority class of their k nearest neighbors.
- Naive Bayes: Probabilistic classifier based on Bayes' theorem with an assumption of feature independence.

Unsupervised Learning Algorithms:
- K-Means Clustering: Partitions data into k clusters where each point belongs to the closest centroid.
- Hierarchical Clustering: Creates a tree of clusters by recursively merging or splitting groups.
- Principal Component Analysis (PCA): Reduces dimensionality by finding principal components that capture maximum variance.
- Association Rules: Discovers interesting relationships between variables in large datasets.
- Anomaly Detection: Identifies rare items, events or observations that differ significantly from the majority.

Ensemble Methods:
- Bagging: Builds multiple models on random subsets of the data and aggregates their predictions.
- Boosting: Sequentially builds models that focus on correcting the errors of previous ones.
- Stacking: Combines predictions from multiple models using another model as a meta-learner.
- Gradient Boosting Machines (GBM): Builds trees sequentially, with each correcting the errors of the ensemble so far.
- XGBoost: Optimized implementation of gradient boosting with regularization to prevent overfitting.


EVALUATION METRICS IN MACHINE LEARNING
-------------------------------------

Classification Metrics:
- Accuracy: The proportion of correct predictions among the total number of predictions.
- Precision: The proportion of true positive predictions among all positive predictions.
- Recall (Sensitivity): The proportion of true positive predictions among all actual positives.
- F1 Score: The harmonic mean of precision and recall, providing a balance between both.
- Specificity: The proportion of true negative predictions among all actual negatives.
- ROC Curve: A plot of the true positive rate against the false positive rate at various threshold settings.
- AUC (Area Under the ROC Curve): A measure of the model's ability to distinguish between classes.

Regression Metrics:
- Mean Absolute Error (MAE): The average of absolute differences between predicted and actual values.
- Mean Squared Error (MSE): The average of squared differences between predicted and actual values.
- Root Mean Squared Error (RMSE): The square root of MSE, providing an error in the same units as the target variable.
- R-squared (Coefficient of Determination): The proportion of the variance in the dependent variable that is predictable from the independent variables.
- Mean Absolute Percentage Error (MAPE): The average of percentage differences between predicted and actual values.

Cross-Validation Techniques:
- K-Fold Cross-Validation: Splits the data into k subsets and trains the model k times, each time using a different subset as the test set.
- Stratified K-Fold: Ensures that each fold maintains the same proportion of class labels as the original dataset.
- Leave-One-Out Cross-Validation (LOOCV): Uses a single observation for testing and the rest for training in each iteration.
- Time Series Cross-Validation: Specialized technique for time series data that respects the temporal order of observations.

Confusion Matrix:
A table used to describe the performance of a classification model on test data where true values are known:
- True Positive (TP): Correctly predicted positive cases
- False Positive (FP): Incorrectly predicted positive cases (Type I error)
- True Negative (TN): Correctly predicted negative cases
- False Negative (FN): Incorrectly predicted negative cases (Type II error) 