<!DOCTYPE html>
<html>
<head>
    <title>ML Algorithms - ML Chat</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 600px;
            margin: 20px auto;
            padding: 10px;
            background-color: #f0f0f0;
        }
        h1 {
            color: #0066cc;
            text-align: center;
        }
        .content {
            padding: 15px;
            background-color: #ffffff;
            border-radius: 5px;
            border: 1px solid #ddd;
            margin-bottom: 20px;
        }
        a.back {
            display: block;
            text-align: center;
            margin-top: 20px;
            padding: 10px;
            background-color: #0066cc;
            color: white;
            text-decoration: none;
            border-radius: 4px;
        }
        a.back:hover {
            background-color: #0055aa;
        }
    </style>
</head>
<body>
    <h1>Machine Learning Algorithms</h1>
    
    <div class="content">
        <h2>Supervised Learning Algorithms</h2>
        <ul>
            <li><strong>Linear Regression:</strong> Predicts a continuous output based on input features using a linear relationship.</li>
            <li><strong>Logistic Regression:</strong> Used for binary classification, estimates the probability of an instance belonging to a class.</li>
            <li><strong>Decision Trees:</strong> Models decisions as a tree of choices that lead to outcomes.</li>
            <li><strong>Random Forests:</strong> Ensemble method combining multiple decision trees to improve accuracy and reduce overfitting.</li>
            <li><strong>Support Vector Machines (SVM):</strong> Finds the optimal hyperplane that separates classes in the feature space.</li>
            <li><strong>K-Nearest Neighbors (KNN):</strong> Classifies data points based on the majority class of their k nearest neighbors.</li>
            <li><strong>Naive Bayes:</strong> Probabilistic classifier based on Bayes' theorem with an assumption of feature independence.</li>
        </ul>
        
        <h2>Unsupervised Learning Algorithms</h2>
        <ul>
            <li><strong>K-Means Clustering:</strong> Partitions data into k clusters where each point belongs to the closest centroid.</li>
            <li><strong>Hierarchical Clustering:</strong> Creates a tree of clusters by recursively merging or splitting groups.</li>
            <li><strong>Principal Component Analysis (PCA):</strong> Reduces dimensionality by finding principal components that capture maximum variance.</li>
            <li><strong>Association Rules:</strong> Discovers interesting relationships between variables in large datasets.</li>
            <li><strong>Anomaly Detection:</strong> Identifies rare items, events or observations that differ significantly from the majority.</li>
        </ul>
        
        <h2>Ensemble Methods</h2>
        <ul>
            <li><strong>Bagging:</strong> Builds multiple models on random subsets of the data and aggregates their predictions.</li>
            <li><strong>Boosting:</strong> Sequentially builds models that focus on correcting the errors of previous ones.</li>
            <li><strong>Stacking:</strong> Combines predictions from multiple models using another model as a meta-learner.</li>
            <li><strong>Gradient Boosting Machines (GBM):</strong> Builds trees sequentially, with each correcting the errors of the ensemble so far.</li>
            <li><strong>XGBoost:</strong> Optimized implementation of gradient boosting with regularization to prevent overfitting.</li>
        </ul>
    </div>
    
    <a href="basic_chat.html" class="back">Back to Chat</a>
</body>
</html> 